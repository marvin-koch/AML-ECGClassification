{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from biosppy.signals import ecg\n",
    "from scipy.signal import periodogram\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, RandomForestRegressor, GradientBoostingClassifier\n",
    "import pywt\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import euclidean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('X_train.csv')\n",
    "X_test_full = pd.read_csv('X_test.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "\n",
    "X_train = X_train.drop('id', axis = 1)\n",
    "X_test = X_test_full.drop('id', axis = 1)\n",
    "y_train = y_train.drop('id', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(ecg_sig, sampling_rate):\n",
    "    X = list()\n",
    "    ts, filtered, rpeaks, templates_ts, templates, heart_rate_ts, heart_rate = ecg.ecg(ecg_sig, sampling_rate, show = False)\n",
    "    \n",
    "    '''\n",
    "    Correct R-peak locations to the maximum within a tolerance\n",
    "    '''\n",
    "    rpeaks = ecg.correct_rpeaks(signal = ecg_sig, rpeaks=rpeaks, sampling_rate = sampling_rate, tol=0.01)  \n",
    "    \n",
    "    '''\n",
    "    extracting values of R-peaks -- Note: rpeaks gives only indices for R-peaks location\n",
    "    '''\n",
    "    peaks = ecg_sig[rpeaks]\n",
    "    \n",
    "    if len(heart_rate) < 2:\n",
    "        heart_rate = [0, 1]\n",
    "    if len(heart_rate_ts) < 2:\n",
    "        heart_rate_ts = [0, 1]\n",
    "           \n",
    "    X = [\n",
    "    np.mean(peaks),\n",
    "    np.median(peaks),\n",
    "    np.min(peaks),\n",
    "    np.max(peaks),\n",
    "    np.std(peaks),\n",
    "    np.mean(rpeaks),\n",
    "    np.median(rpeaks),\n",
    "    np.min(rpeaks),\n",
    "    np.max(rpeaks),\n",
    "    np.std(rpeaks),\n",
    "    np.mean(np.diff(rpeaks)),\n",
    "    np.median(np.diff(rpeaks)),\n",
    "    np.min(np.diff(rpeaks)),\n",
    "    np.max(np.diff(rpeaks)),\n",
    "    np.std(np.diff(rpeaks)),\n",
    "    np.mean(heart_rate),\n",
    "    np.median(heart_rate),\n",
    "    np.min(heart_rate),\n",
    "    np.max(heart_rate),\n",
    "    np.std(heart_rate),\n",
    "    np.mean(np.diff(heart_rate)),\n",
    "    np.median(np.diff(heart_rate)),\n",
    "    np.min(np.diff(heart_rate)),\n",
    "    np.max(np.diff(heart_rate)),\n",
    "    np.std(np.diff(heart_rate)),\n",
    "    np.mean(heart_rate_ts),\n",
    "    np.median(heart_rate_ts),\n",
    "    np.min(heart_rate_ts),\n",
    "    np.max(heart_rate_ts),\n",
    "    np.std(heart_rate_ts),\n",
    "    np.mean(np.diff(heart_rate_ts)),\n",
    "    np.median(np.diff(heart_rate_ts)),\n",
    "    np.min(np.diff(heart_rate_ts)),\n",
    "    # np.min(np.diff(heart_rate_ts)),  # Double check this line, it seems duplicated in your original code\n",
    "    np.max(np.diff(heart_rate_ts)),\n",
    "    np.std(np.diff(heart_rate_ts)),\n",
    "    np.sum(filtered - ecg_sig)\n",
    "    ]\n",
    "    \n",
    "    X += list(np.mean(templates, axis=0))\n",
    "    X += list(np.median(templates, axis=0))\n",
    "    X += list(np.std(templates, axis=0))\n",
    "    X += list(np.min(templates, axis=0))\n",
    "    X += list(np.max(templates, axis=0))\n",
    "    # coefficients = pywt.wavedec(np.mean(templates, axis=0), 'db1', level=5)\n",
    "    # wavelet_features = [item for sublist in coefficients for item in sublist]\n",
    "    # X += wavelet_features[:45]  # Limit the number of wavelet features\n",
    "    \n",
    "    # # Compute PCA and add the first few principal components as features\n",
    "    # pca = PCA(n_components=5)\n",
    "    # pca.fit(templates)\n",
    "    # pca_features = pca.transform(templates)\n",
    "    # pca_features = pca_features.flatten()[:45]  # Flatten and take the first few features\n",
    "    # X += list(pca_features)\n",
    "    \n",
    "    X = np.array(X)\n",
    "    \n",
    "    # X[np.isnan(X)] = 0\n",
    "    return X\n",
    "\n",
    "train = []\n",
    "for i in range(X_train.shape[0]):\n",
    "    x = np.array(X_train.loc[i].dropna())\n",
    "    features = extract_features(x, 300)\n",
    "    train.append(features)\n",
    "\n",
    "test = []\n",
    "for i in range(X_test.shape[0]):\n",
    "    x = np.array(X_test.loc[i].dropna())\n",
    "    features = extract_features(x, 300)\n",
    "    test.append(features)\n",
    "\n",
    "y = np.ravel(np.array(y_train.values))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct, WhiteKernel\n",
    "import numpy as np\n",
    "from playsound import playsound\n",
    "\n",
    "\n",
    "# Create a Gaussian Process Classifier\n",
    "classifier = GaussianProcessClassifier(random_state=0)\n",
    "\n",
    "# Create a time series split\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Define the scoring metric\n",
    "scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "grid_search = GridSearchCV(classifier, param_grid=param_grid, cv=tscv, scoring=scorer, n_jobs=-1,verbose=3)\n",
    "grid_search.fit(train, y)\n",
    "\n",
    "\n",
    "# Print the best parameters and corresponding mean cross-validated score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Mean F1 Score:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.82297155 0.82613277 0.82613277 0.82086407 0.82489451]\n",
      "0.824199134776558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import make_scorer, f1_score\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, ExpSineSquared, DotProduct, WhiteKernel\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import numpy as np\n",
    "from playsound import playsound\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, chi2\n",
    "from scipy import stats\n",
    "\n",
    "imp = IterativeImputer(n_nearest_features=15, max_iter=50)\n",
    "\n",
    "train = imp.fit_transform(train)\n",
    "test = imp.transform(test)\n",
    "\n",
    "sc = RobustScaler(quantile_range=(0.3,0.7))\n",
    "\n",
    "train_nor = sc.fit_transform(train, y)\n",
    "\n",
    "z_scores = np.abs(stats.zscore(train_nor))\n",
    "mask = ~(np.any(z_scores > 7, axis=1))\n",
    "train_nor = pd.DataFrame(train_nor[mask])\n",
    "y_masked = pd.DataFrame(np.ravel(y[mask]))\n",
    "\n",
    "test_nor = sc.transform(test)\n",
    "classifier = XGBClassifier(n_jobs=-1, eval_metric='merror', n_estimators=1000, max_depth=5, learning_rate=0.1)\n",
    "\n",
    "scorer = make_scorer(f1_score, average='micro')\n",
    "tscv = TimeSeriesSplit(n_splits=7)\n",
    "\n",
    "res = (cross_val_score(classifier, train_nor, y_masked, scoring=scorer, n_jobs=-1))\n",
    "print(res)\n",
    "print(np.mean(res))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_nor,y_masked)\n",
    "y_test = classifier.predict(test_nor)\n",
    "submission = pd.DataFrame({'id': X_test_full['id'], 'y': y_test})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
